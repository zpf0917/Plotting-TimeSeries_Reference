{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FUNCS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BINPLOT_VA1_parted_by_categorical_VA2(\n",
    "        df, \n",
    "        va1, \n",
    "        va2, \n",
    "        ls = None,\n",
    "\n",
    "        whether_plot = True,\n",
    "        fig_size = (16,4)\n",
    "    ): \n",
    "    \"\"\"\n",
    "    Params:\n",
    "        \n",
    "    Returns: \n",
    "        DataFrame of means, stds & counts\n",
    "    Note:\n",
    "        1. Check va2 is categorical\n",
    "        2. Require no repeating index, for time series index, \n",
    "            consider push repeating index by 1ns, repeat \n",
    "            until no repetition left.\n",
    "            \n",
    "    TODO: extend to multiple categorical va2 values\n",
    "    TODO: allow multiple category values to take one bin!!!!!!\n",
    "    \"\"\"\n",
    "        \n",
    "    if ls == None:\n",
    "        ls = list(df[va2].unique())\n",
    "    \n",
    "    # groupby / partition\n",
    "    df_temp = df[(df[va2].apply(lambda x: (x in ls)))]\n",
    "    unstacked = df_temp.groupby(va2).apply(lambda x: x[va1]).unstack(level = 0)\n",
    "\n",
    "    # plot\n",
    "    if whether_plot:\n",
    "        ax = unstacked.plot(kind='box',figsize = fig_size)\n",
    "        _ = ax.set_xlabel(\"CATEGORY of {\" + va2 + \"}\")\n",
    "        _ = ax.set_ylabel(va1)\n",
    "    \n",
    "#     # return\n",
    "    ls_means = list(unstacked.mean(axis = 0, skipna = True))\n",
    "    ls_stds = list(unstacked.std(axis = 0, skipna = True))\n",
    "    ls_counts =list((-1*(unstacked.isna()-1)).sum())\n",
    "    df_ret = pd.DataFrame(data = ([ls_means, ls_stds,ls_counts]),\n",
    "                          index = ['mean','std','count'],\n",
    "                          columns = unstacked.columns)\n",
    "    df_ret = df_ret.transpose()\n",
    "    df_ret.rename_axis(\"CATEGORY of {\" + va2 + \"}\", inplace = True)\n",
    "    return df_ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BINPLOT_VA1_parted_by_list_of_pairs_of_VA2(\n",
    "        df, \n",
    "        va1, \n",
    "        va2, \n",
    "        ls,\n",
    "        va2_is_index = False, \n",
    "\n",
    "        whether_plot = True,\n",
    "        fig_size = (16,4),\n",
    "        show_x_ticks = True\n",
    "    ): \n",
    "    \"\"\"\n",
    "    Params:\n",
    "        【va2_is_index】 True:\n",
    "                Then 【va2】 is just a name of x axis\n",
    "    Returns: \n",
    "        DataFrame of means, stds & counts\n",
    "    Note:\n",
    "        1. Check no overlapping region! Region is LEFT <= x < RIGHT\n",
    "        2. Require no repeating index, for time series index, \n",
    "            consider push repeating index by 1ns, repeat \n",
    "            until no repetition left.\n",
    "    \"\"\"\n",
    "    \n",
    "    # setup\n",
    "    UNIQUE_ID_COL = 'unique_group_identifier_to_be_deleted_by_nathan_3434344' \n",
    "    UNIQUE_VA2_INDEX_COL = 'unique_va2_index_to_be_deleted_by_nathan_3434344' \n",
    "    STORE_va2 = va2\n",
    "    if va2_is_index:\n",
    "        va2 = UNIQUE_VA2_INDEX_COL\n",
    "        df[va2] = df.index\n",
    "    \n",
    "    # groupby / partition\n",
    "    df[UNIQUE_ID_COL] = -1\n",
    "    counter = 0\n",
    "    for pair in ls:\n",
    "        left, right = pair[0], pair[1]\n",
    "        if (left != None and right != None): df.loc[(left <= df[va2]) & (df[va2] < right), UNIQUE_ID_COL] = counter\n",
    "        elif (left == None): df.loc[(df[va2] < right), UNIQUE_ID_COL] = counter\n",
    "        elif (right == None): df.loc[(left <= df[va2]), UNIQUE_ID_COL] = counter\n",
    "        else : df[UNIQUE_ID_COL] = counter\n",
    "        counter += 1\n",
    "    df_temp = df[df[UNIQUE_ID_COL] != -1]\n",
    "    unstacked = df_temp.groupby(UNIQUE_ID_COL).apply(lambda x: x[va1]).unstack(level = 0)\n",
    "\n",
    "    # plot\n",
    "    if whether_plot:\n",
    "        ax = unstacked.plot(kind='box',figsize = fig_size)\n",
    "        if show_x_ticks:\n",
    "            _ = ax.set_xticklabels(unstacked.columns)\n",
    "        _ = ax.set_xlabel(\"RANGE of {\" + STORE_va2 + \"}\")\n",
    "        _ = ax.set_ylabel(va1)\n",
    "    \n",
    "    # restore original df\n",
    "    df.drop(UNIQUE_ID_COL, axis = 1, inplace = True)\n",
    "    if va2_is_index: \n",
    "        df.drop(UNIQUE_VA2_INDEX_COL, axis = 1, inplace = True)\n",
    "    \n",
    "    # return\n",
    "    ls_means = list(unstacked.mean(axis = 0, skipna = True))\n",
    "    ls_stds = list(unstacked.std(axis = 0, skipna = True))\n",
    "    ls_counts =list((-1*(unstacked.isna()-1)).sum())\n",
    "    columns_ls = list(map(lambda x: \"[\" + str(x[0]) +\", \" + str(x[1]) + \")\", ls))\n",
    "    df_ret = pd.DataFrame(data = ([ls_means, ls_stds,ls_counts]),\n",
    "                          index = ['mean','std','count'],\n",
    "                          columns = columns_ls)\n",
    "    df_ret = df_ret.transpose()\n",
    "    df_ret.rename_axis(\"RANGE of {\" + STORE_va2 + \"}\", inplace = True)\n",
    "    return df_ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('train.csv')\n",
    "df.drop('Name', axis = 1, inplace = True)\n",
    "\n",
    "def Convert_sex(x):\n",
    "    if x == 'male':\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "df['Sex'] = df['Sex'].apply(lambda x: Convert_sex(x))\n",
    "\n",
    "df.drop('PassengerId', axis = 1, inplace = True)\n",
    "df.drop('Ticket', axis =1, inplace = True)\n",
    "\n",
    "def ConvertCabin(x):\n",
    "    if pd.isna(x):\n",
    "        return 'NAN'\n",
    "    else:\n",
    "        return x[0]\n",
    "df['Cabin'] = df['Cabin'].apply(lambda x: ConvertCabin(x))\n",
    "\n",
    "def Create_DUMMY(\n",
    "    df,\n",
    "    va1\n",
    "):\n",
    "    ls = sorted(df[va1].unique())\n",
    "    for i in ls[:-1]:\n",
    "        str_temp = va1 + '___' + str(i)\n",
    "        df[str_temp] = 0\n",
    "        df.loc[df[va1] == i, str_temp] = 1\n",
    "    df.drop(va1, axis =1, inplace = True)\n",
    "    \n",
    "df.loc[df['Age'].isnull(), 'Age'] = df['Age'].mean()\n",
    "df.loc[df['Embarked'].isnull(), 'Embarked'] = df['Embarked'].mode().iloc[0]\n",
    "\n",
    "Create_DUMMY(df, 'Pclass')\n",
    "Create_DUMMY(df, 'Cabin')\n",
    "Create_DUMMY(df, 'Embarked')\n",
    "\n",
    "df['y'] = df['Survived']\n",
    "df.drop('Survived', axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train-test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_rand = np.random.choice(a = [True, False],p = [0.7, 0.3], size = df.shape[0])\n",
    "test_rand = train_rand == False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df[train_rand].copy()\n",
    "df_test = df[test_rand].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "INDEX = ['Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Pclass___1', 'Pclass___2',\n",
    "       'Cabin___A', 'Cabin___B', 'Cabin___C', 'Cabin___D', 'Cabin___E',\n",
    "       'Cabin___F', 'Cabin___G', 'Cabin___NAN', 'Embarked___C', 'Embarked___Q']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df_train[INDEX]\n",
    "y_train = df_train['y']\n",
    "\n",
    "X_test = df_test[INDEX]\n",
    "y_test = df_test['y']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SKLEARN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zhaopf/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression(random_state=0).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.826580226904376"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# y_train_pred = clf.predict(X_train) \n",
    "y_train_pred = clf.predict_proba(X_train)[:, 1] > 0.5\n",
    "(y_train_pred == y_train).sum() / y_train.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.781021897810219"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_pred = clf.predict(X_test)\n",
    "(y_test_pred == y_test).sum() / y_test.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare for C++\n",
    "* Retrieve coef & bias, then get probability\n",
    "* COEF & BIAS pass to 【.conf】, formula of prob implement in 【.cc】 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "COEF = clf.coef_\n",
    "BIAS = clf.intercept_\n",
    "clf.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.92724499]\n",
      "[0.92724499 0.07275501]\n"
     ]
    }
   ],
   "source": [
    "raw_val = X_test.iloc[0].values @ COEF.reshape(-1,1) + BIAS\n",
    "actual_prob = np.exp(-1 * raw_val)  / (np.exp(-1 * raw_val) + np.exp(0))\n",
    "print(actual_prob)\n",
    "print(clf.predict_proba(X_test)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Y_hat VS true prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['sklearn_raw'] = df_test[INDEX].values @ COEF.reshape(-1,1) + BIAS\n",
    "df_test['sklearn_prob'] = np.exp(-1 * df_test['sklearn_raw'])  / (np.exp(-1 * df_test['sklearn_raw']) + np.exp(0))\n",
    "df_test['sklearn_prob'] = 1 - df_test['sklearn_prob']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RANGE of {sklearn_prob}</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>[None, 0.2)</th>\n",
       "      <td>0.169492</td>\n",
       "      <td>0.376785</td>\n",
       "      <td>118.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>[0.5, 0.7)</th>\n",
       "      <td>0.627907</td>\n",
       "      <td>0.489083</td>\n",
       "      <td>43.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>[0.8, 1)</th>\n",
       "      <td>0.906977</td>\n",
       "      <td>0.293903</td>\n",
       "      <td>43.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             mean       std  count\n",
       "RANGE of {sklearn_prob}                           \n",
       "[None, 0.2)              0.169492  0.376785  118.0\n",
       "[0.5, 0.7)               0.627907  0.489083   43.0\n",
       "[0.8, 1)                 0.906977  0.293903   43.0"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BINPLOT_VA1_parted_by_list_of_pairs_of_VA2(\n",
    "        df = df_test, \n",
    "        va1 = 'y', \n",
    "        va2 = 'sklearn_prob', \n",
    "        ls = [[None, 0.2],[0.5,0.7],[0.8,1]],\n",
    "        va2_is_index = False, \n",
    "\n",
    "        whether_plot = False,\n",
    "        fig_size = (16,4),\n",
    "        show_x_ticks = True\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TORCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticClassifier(nn.Module):\n",
    "    # output_dim MATCHES whether_binary\n",
    "        # binary ==> output dim = 2\n",
    "        # n-classes ==> output dim = n\n",
    "    \n",
    "    def __init__(self, input_dim, hidden_dim, output_dim = 2):\n",
    "        super(LogisticClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "        [self.W1, self.B1] = list(self.fc1.parameters())\n",
    "        [self.W2, self.B2] = list(self.fc2.parameters())\n",
    "\n",
    "    def forward(self, x_in, whether_predict = False):\n",
    "        a_1 = self.fc1(x_in)\n",
    "        a_1 = torch.relu(a_1)\n",
    "        y_pred = self.fc2(a_1)\n",
    "        if whether_predict:\n",
    "            y_pred = F.softmax(y_pred, dim=1)    # dim ???\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method Module.named_modules of LogisticClassifier(\n",
      "  (fc1): Linear(in_features=17, out_features=5, bias=True)\n",
      "  (fc2): Linear(in_features=5, out_features=2, bias=True)\n",
      ")>\n"
     ]
    }
   ],
   "source": [
    "model = LogisticClassifier(input_dim = X_train.shape[1], \n",
    "                           hidden_dim = 5, \n",
    "                           output_dim = 2)\n",
    "print (model.named_modules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "iter = 0\n",
    "for epoch in range(100000):\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    inputs = Variable(torch.tensor(X_train.values, dtype = torch.float32))\n",
    "    labels = Variable(torch.tensor(y_train.values, dtype = torch.long))\n",
    "    \n",
    "    \n",
    "    outputs = model(inputs)\n",
    "    loss = criterion(outputs, labels)\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    iter += 1\n",
    "    if iter % 1000 == 0:\n",
    "        test_inputs = Variable(torch.tensor(X_test.values, dtype = torch.float32))\n",
    "        test_labels = Variable(torch.tensor(y_test.values, dtype = torch.long))\n",
    "        outputs = model(test_inputs, whether_predict = True)\n",
    "        _, predicted = torch.max(outputs.data, dim = 1)\n",
    "        total = test_labels.size(0)\n",
    "        correct = (predicted == test_labels).sum()\n",
    "        print(100 * correct/total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ALGO\n",
    "### At anytime: no huge imbalance\n",
    "* 【Hard margin】 keep an imbalance variable, abs() of which, if approaching certain bound, nothing that will make it cross this bound is allowed.\n",
    "    * How big of a margin? How to decide?\n",
    "    * Smaller than or equal to the \"warning limit\" by a factor\n",
    "* 【Soft margin】 keep a counter variable (regarding trade volumes), make every addition to this counter increasingly hard \n",
    "    * Make sure that this counter should zero out when there is no imbalance ==> choose trade volume as subject (not # trades)\n",
    "    * This counter variable can be used to decide \"how much to trade each time\"\n",
    "* 【Hybrid margin】 within a \"hard semi-margin\", we can do anything; outside this margin, we start getting more and more \"reluctant\" to get more imbalanced.\n",
    "\n",
    "* 【Reverse hybrid margin】 \"reluctancy\" soft margin always happens, but there is an outer hard margin that one can never cross.\n",
    "    * This is may be the way to go, since it inherits the good properties of soft margin == efficiently allocate buy size based on \"reluctancy\" & it will never trigger \"huge imbalance\" warning by setting hard margin to be the actual \"warning limit\".\n",
    "    \n",
    "* 【Triple hybrid margin】 \n",
    "    * IF WE CAN set the soft margin's \"reluctancy constant\" to gently adhere to the actual \"warning limit\", the above approach is not really that \"hybrid\", & soft margin that does not adhere well is not an efficient soft margin.\n",
    "    * We need a hard \"freedom margin\", with an imbalance underneath which, one can do anything.\n",
    "    * We have a soft margin kicking in above this \"freedom margin\" that gently adhere to the \"warning limit\"\n",
    "    * We add a hard \"dead margin\" that is the actual \"warning limit\" just to make sure.\n",
    "\n",
    "### End of day: no position\n",
    "\n",
    "##### Time in day based:\n",
    "1. 18 hours to trade based on any kind of margin\n",
    "2. 5 hours to hopefully tie loose ends:\n",
    "    * For new opportunities: \"able to buys at px that will rise\" / \"able to short at px that will drop\", we only take those that may reduce imbalance. \n",
    "        * (Since \"reluctancy score\" is on the other side, we would natually take these quite willingly, BUT here, we say that we only take these)\n",
    "        * When these \"balancing new opportunities\" are captured: two cases, one we make profit, but imbalance remains, two the opportunity half-completes, we reduce imbalance.\n",
    "        \n",
    "    * Reflection: how did we arrive at this point of imbalance.?\n",
    "        * Say: we are in a state where we hold a lot of BTC\n",
    "        * We must have bought BTC a lot, with regression telling me that px would rise in 30s, we bought BTC instantly, and set a limit order at a higher price, hoping it would get picked up, it did not. \n",
    "        * This means that, at later time than that initial trade, the px never rised to the point of the limit order.\n",
    "        * That AOC order might mark a peak of the entire day's trading: \"We have bought at the high peak of the day\", SHIT!!!\n",
    "        * Takeaway 1: Want to AVOID peak, with some kind of mechanism. Want to be a part of any slope. (NOT TOO DO-ABLE)\n",
    "        * Having a ton of imbalance, means that, we have bought at a lot of peaks / bought once at peak with large amount ==> the day has not been a very volatile (with large fluctuation) day (or hours) in general.\n",
    "        * Takeaway 2: For less volatile days (or hours), we are more likely to get on lots of peaks on one side, need a tighter \"freedom margin\", soft margin, and \"dead margin\".\n",
    "            * We have the volatility in 30s thing, as \"y\" of regression (need log since FutureStd always positive)\n",
    "            * We might need some \"Volatility in past\" signal, or just use existing 5 past px signals in the regression.\n",
    "            * If see \"futureStd\" is large, more willing to initiate an opportunity (larger amount)\n",
    "        \n",
    "    * Mode recognition: if we balances imbalance earlier than 6 hour finishes?\n",
    "        * TEMPORARY: Cease trading & cancel hanging orders that might create new imbalances.\n",
    "        * If lots of these happens, want to shorten the previously determined 5 hours.\n",
    "    \n",
    "3. 1 hours to tie loose ends at cost of profit loss\n",
    "    * \n",
    "\n",
    "    \n",
    "### General logic of Trade size (first 18h)\n",
    "* More volatile in 30s (larger \"futureStd\"), larger amount <== less scared to get stuck\n",
    "* Having had huge imbalance on the same side, less amount <== chance to worsen the current imbalance\n",
    "* Having had imbalance on the other side, larger amount (not 2 times larger though) <== chance to alleviate the current imbalance\n",
    "* Larger regressed change in px, larger amount\n",
    "* Larger classification prob of \"profit making change\", larger amount (based on Kelly Criterion?)\n",
    "\n",
    "### Need to check out for return info from limit orders, have a \"floating imbalance\" \n",
    "* If a limit order fails on exchange side, send a same order with worse price (MAYBE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
